% !TEX encoding = UTF-8 Unicode
\documentclass[10pt, a4paper]{article}


\usepackage{footnote}
\makesavenoteenv{tabular}
\usepackage{ltc05}
%\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{textcomp} 
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{url}
\title{Sentiment Analysis}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% PLEASE DO NOT WRITE YOUR NAME AND ADDRESS IN THE DRAFT OF YOUR PAPER
% SIMPLY ERASE THE LINES...
\name{ML$^{\dagger}$\\ \bf \large PęP$^{\ast}$} 

\address{ $^{\dagger}$VoiceLab \\	
               michal.lew@voicelab.pl \\ \\
               $^{\ast}$University of Lodz \\ 
               piotr.pezik@uni.lodz.pl}

%% ... UP TO HERE
% ... UP TO HERE
%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\abstract{This paper evaluates thsdfh.}

\begin{document}

\maketitleabstract

\section{Introduction}   
Sentiment Analysis (SA) is an active area of natural language processing with applications in general opinion mining, customer relations management systems, marketing and social media communication studies. A common objective of SA is to automatically detect the attitudinal value of an utterance or otherwise coherent stretch of text which can be attributed to a particular author. The distribution of  such attitudinal values in an utterance is usually known as its \textit{polarity} as it usually ranges from positive to neutral and negative.\cite{cambria_schuller}. More recent approaches to SA stress the importance of phrase-level polarity classification, whereby attitudinal values are detected at the level of syntactic phrases and subsequently compositionally combined to obtain the polarity of an entire utterance. Such approaches are particulary important in aspect-based SA, where different aspects of a product or other objects of one's opinions are important to detect and classify. Also, phrase-level SA is better suited to deal with basic syntactic operations such as negation or modality which may cause a significant shift in the predominant sentiment of an utterance.\cite{wilson_wiebe}. 

\section{Previous work}




\section{The Data}
\subsection{PolEval 2017 Dataset}

Nanan 


\section{Model} 


\subsection{LSTM}

Recurrent Neural Networks has been widely used for many task in natural language processing. They can process the arbitrary long inputs by recurrent application of transition function over hidden state. 
	The most common form of RNN's transition function is an affine transformation followed by hyperbolic tangent function
	\begin{equation} h_t = tanh(W_{x_t}+U_{h_{t-1}}+b)
\end{equation}
	  	This gives them ability to use the information gathered in previous states when processing text. In theory it makes them able to look at the infinite history while processing sequences of words. 	Unfortunately they suffer from so called vanishing gradient problem, which means that during training gradient can grow or decay exponentially over long sequences [s (Hochreiter,
1998; Bengio et al., 1994)]
	The LSTM architecture (Hochreiter and Schmidhuber, 1997) addresses this problem of learning long-term dependencies by introducing a memory cell that is able to preserve state over long periods of time. [citation]
		There are many different variants of LSTMs, in our work we used following equations:
		[równania LSTM]		
		where $x_t$ is input at current timestep, $i_t$ is an input gate, $f_t$ a forget gate, $o_t$ an output gate and $c$ stands for LSTM memory.
		The gates mechanism controles how much information from past state and memory is used at current timestep. 

\subsection{Tree-LSTM}
	The problem with classical LSTM's is that they are linear chain which makes them useless for processing more sophisticated structures than sequences. In particular, many linguistic beings are represented with tree-structure [citation?].
	In order to process tree-structured data we need to extend the LSTM architecture. In our work we propose Sequential-Child-Combination Tree-LSTM.
	In general, tree-structured Recurrent Neural Networks expand basic RNNs by creating a way to plug in more than one previous state to hidden unit at current time step.
	[obrazek podłączanie 2 poprzednich stanów do nowego]
	The essence lie in the way in which the children states are combined with parent state.

\subsection{Child-Sum Tree-LSTM}
        Child-Sum Tree-LSTM equations
	
\subsection{Sequential-Child-Combination Tree-LSTM}
	In our work we combine children with its parent as follows:
	
\section{Evaluation} 

\begin{table}[h]
 \begin{center}
\begin{tabular}{|l|l|}

      \hline
      %Level&Tools\\
      %\hline\hline
      Number of tags (without \textit{ign}) & 35\\
      \hline
      Token accuracy & 98.2\%\\
      \hline
      Training set & PolEval TR\\
      \hline
      Test set & PolEval TE\\
           \hline
      Training time & 5m 40s \\
      &(5 iterations)\\
      \hline
      Tagging time & 1.42s\\
 
      \hline
\end{tabular}
\caption{Speed and accuracy evaluation of an AP-based `flexeme' tagger.}
\label{tab:main_tags}
 \end{center}
\end{table}



\section{Conclusions and future work}


\section{Availability}

The source code of the tagger described in this paper, together with the trained models, datasets, tagset translations and other resources such as the NCP Brown clusters are publicly available at \url{https://gitlab.com/piotr.pezik/apt_pl}.

%\nocite{*}

\bibliographystyle{ltc05}
\bibliography{sal} 

\end{document}

