% !TEX encoding = UTF-8 Unicode
\documentclass[10pt, a4paper]{article}


\usepackage{footnote}
\makesavenoteenv{tabular}
\usepackage{ltc05}
%\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{textcomp} 
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{url}
\title{A Sequential Child-Combination Tree-LSTM Network for Sentiment Analysis}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% PLEASE DO NOT WRITE YOUR NAME AND ADDRESS IN THE DRAFT OF YOUR PAPER
% SIMPLY ERASE THE LINES...
\name{ML$^{\dagger}$\\ \bf \large PęP$^{\ast}$} 

\address{ $^{\dagger}$VoiceLab \\	
               michal.lew@voicelab.pl \\ \\
               $^{\ast}$University of Lodz \\ 
               piotr.pezik@uni.lodz.pl}

%% ... UP TO HERE
% ... UP TO HERE
%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\abstract{This paper evaluates thsdfh.}

\begin{document}

\maketitleabstract

\section{Introduction}   
Sentiment Analysis (SA) is an active area of natural language processing research with applications in general opinion mining, customer relations management systems, marketing and social media communication studies to mention just a few examples. A common objective of SA is to automatically detect the attitudinal value of an utterance or otherwise coherent stretch of text which can be attributed to a particular author. The distribution of  such attitudinal values in an utterance is usually known as its \textit{polarity} and it usually ranges from positive to neutral and negative \cite{cambria_schuller}. More recent approaches to SA focus on phrase-level polarity classification, whereby attitudinal values are detected at the level of syntactic phrases and only then compositionally combined to compute the polarity of an entire utterance. Such approaches are particularly important in aspect-based SA, where different aspects of one's opinion about a product, movie, person, etc. have to be detected in addition to  classifying the overall sentiment of a text unit. Also, phrase-level SA is better suited to deal with basic syntactic phenomena such as negation or modality which may cause a significant shift in the predominant sentiment of an utterance \cite{wilson_wiebe}. For example, the overall polarity of the Polish sentence shown in Table \ref{tab:nie_drazni_1}, which describes a buyer's overall satisfaction with a particular brand of perfume, is positive, even though its (directly negated) main verb \textit{drażnić} (irritate) could be found in a list of keywords denoting negative sentiment. 
\par The present paper evaluates a deep-learning based system for to syntax-driven SA submitted to the PolEval 2017 competition. We first briefly describe the syntactically annotated sentiment datasets used and the exact nature of the SA classification task. Next, we present the Sequential Child-Combination Tree-LSTM Network designed to implement our system and evaluate its performance on the PolEval test set.


\section{PolEval 2017}


\begin{table}[h]
 \begin{center}
\begin{tabular}{|l l l l l l l l|}

      \hline
      %Level&Tools\\
      %\hline\hline
      W & Nie & drażni & nawet & po & całym & dniu & . \\
      \hline
      P & 2 & 0 & 2 & 2 & 6 & 4 & 2 \\
      \hline
      S & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
 
      \hline
\end{tabular}
\caption{An example PolEval dataset sentence with phrase-level sentiment annotation (W -- word nodes, P -- syntactic parents, S -- polarity value).}
\label{tab:nie_drazni_1}
 \end{center}
\end{table}






\section{Model} 


\subsection{LSTM}

Recurrent Neural Networks have been widely used for many task in natural language processing. They can process the arbitrary long inputs by recurrent application of transition function over hidden state. 
	The most common form of RNN's transition function is an affine transformation followed by hyperbolic tangent function
	\begin{equation} h_t = tanh(W_{x_t}+U_{h_{t-1}}+b)
\end{equation}
	  	This gives them ability to use the information gathered in previous states when processing text. In theory it makes them able to look at the infinite history while processing sequences of words. 	Unfortunately they suffer from so called vanishing gradient problem, which means that during training gradient can grow or decay exponentially over long sequences [s (Hochreiter,
1998; Bengio et al., 1994)]
	The LSTM architecture (Hochreiter and Schmidhuber, 1997) addresses this problem of learning long-term dependencies by introducing a memory cell that is able to preserve state over long periods of time. [citation]
		There are many different variants of LSTMs, in our work we used following equations:
		[równania LSTM]		
		where $x_t$ is input at current timestep, $i_t$ is an input gate, $f_t$ a forget gate, $o_t$ an output gate and $c$ stands for LSTM memory.
		The gates mechanism controles how much information from past state and memory is used at current timestep. 

\subsection{Tree-LSTM}
	The problem with classical LSTM's is that they are linear chain which makes them useless for processing more sophisticated structures than sequences. In particular, many linguistic beings are represented with tree-structure [citation?].
	In order to process tree-structured data we need to extend the LSTM architecture. In our work we propose Sequential Child-Combination Tree-LSTM.
	In general, tree-structured Recurrent Neural Networks expand basic RNNs by creating a way to plug in more than one previous state to hidden unit at current time step.
	[obrazek podłączanie 2 poprzednich stanów do nowego]
	The essence lie in the way in which the children states are combined with parent state.

\subsection{Child-Sum Tree-LSTM}
        Child-Sum Tree-LSTM equations
	
\subsection{Sequential-Child-Combination Tree-LSTM}
	In our work we combine children with its parent as follows:
	
\section{Evaluation} 

\begin{table}[h]
 \begin{center}
\begin{tabular}{|l|l|}

      \hline
      %Level&Tools\\
      %\hline\hline
      Number of tags (without \textit{ign}) & 35\\
      \hline
      Token accuracy & 98.2\%\\
      \hline
      Training set & PolEval TR\\
      \hline
      Test set & PolEval TE\\
           \hline
      Training time & 5m 40s \\
      &(5 iterations)\\
      \hline
      Tagging time & 1.42s\\
 
      \hline
\end{tabular}
\caption{Speed and accuracy evaluation of an AP-based `flexeme' tagger.}
\label{tab:main_tags}
 \end{center}
\end{table}



\section{Conclusions and future work}


\section{Availability}

The source code of the tagger described in this paper, together with the trained models, datasets, tagset translations and other resources such as the NCP Brown clusters are publicly available at \url{https://gitlab.com/piotr.pezik/apt_pl}.

%\nocite{*}

\bibliographystyle{ltc05}
\bibliography{sal} 

\end{document}

